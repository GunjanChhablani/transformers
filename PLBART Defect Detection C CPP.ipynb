{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from fairseq.models.bart import BARTModel"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-10-08 02:52:21.336975: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-10-08 02:52:21.337034: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# # MBart Test\n",
    "# mbart = BARTModel.from_pretrained('mbart.cc25.v2', checkpoint_file='model.pt')\n",
    "# mbart.eval()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates.\n",
    "#\n",
    "# This source code is licensed under the MIT license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "import logging\n",
    "import os\n",
    "from argparse import Namespace\n",
    "\n",
    "import numpy as np\n",
    "from fairseq import utils\n",
    "from fairseq.data import (\n",
    "    ConcatSentencesDataset,\n",
    "    Dictionary,\n",
    "    IdDataset,\n",
    "    NestedDictionaryDataset,\n",
    "    NumelDataset,\n",
    "    NumSamplesDataset,\n",
    "    OffsetTokensDataset,\n",
    "    PrependTokenDataset,\n",
    "    RawLabelDataset,\n",
    "    RightPadDataset,\n",
    "    RollDataset,\n",
    "    SortDataset,\n",
    "    StripTokenDataset,\n",
    "    data_utils,\n",
    ")\n",
    "from fairseq.data.shorten_dataset import maybe_shorten_dataset\n",
    "from fairseq.tasks import FairseqTask, register_task\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class LegacyFairseqTask(FairseqTask):\n",
    "    def __init__(self, args: Namespace):\n",
    "        self.args = args\n",
    "        self.datasets = {}\n",
    "        self.dataset_to_epoch_iter = {}\n",
    "\n",
    "    @classmethod\n",
    "    def setup_task(cls, args: Namespace, **kwargs):\n",
    "        \"\"\"Setup the task (e.g., load dictionaries).\n",
    "        Args:\n",
    "            args (argparse.Namespace): parsed command-line arguments\n",
    "        \"\"\"\n",
    "        return cls(args, **kwargs)\n",
    "\n",
    "    def has_sharded_data(self, split):\n",
    "        return os.pathsep in getattr(self.args, \"data\", \"\")\n",
    "\n",
    "    def build_model(self, args: Namespace):\n",
    "        \"\"\"\n",
    "        Build the :class:`~fairseq.models.BaseFairseqModel` instance for this\n",
    "        task.\n",
    "        Args:\n",
    "            args (argparse.Namespace): parsed command-line arguments\n",
    "        Returns:\n",
    "            a :class:`~fairseq.models.BaseFairseqModel` instance\n",
    "        \"\"\"\n",
    "        from fairseq import models, quantization_utils\n",
    "\n",
    "        model = models.build_model(args, self)\n",
    "        if getattr(args, \"tpu\", False):\n",
    "            model.prepare_for_tpu_()\n",
    "        model = quantization_utils.quantize_model_scalar(model, args)\n",
    "        return model\n",
    "\n",
    "    def build_criterion(self, args: Namespace):\n",
    "        \"\"\"\n",
    "        Build the :class:`~fairseq.criterions.FairseqCriterion` instance for\n",
    "        this task.\n",
    "        Args:\n",
    "            args (argparse.Namespace): parsed command-line arguments\n",
    "        Returns:\n",
    "            a :class:`~fairseq.criterions.FairseqCriterion` instance\n",
    "        \"\"\"\n",
    "        from fairseq import criterions\n",
    "\n",
    "        return criterions.build_criterion(args, self)\n",
    "\n",
    "\n",
    "@register_task(\"plbart_sentence_prediction\")\n",
    "class BARTSentencePredictionTask(LegacyFairseqTask):\n",
    "    \"\"\"\n",
    "    Sentence (or sentence pair) prediction (classification or regression) task.\n",
    "    Args:\n",
    "        dictionary (Dictionary): the dictionary for the input of the task\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def add_args(parser):\n",
    "        \"\"\"Add task-specific arguments to the parser.\"\"\"\n",
    "        parser.add_argument(\"data\", metavar=\"FILE\", help=\"file prefix for data\")\n",
    "        parser.add_argument(\n",
    "            \"--num-classes\",\n",
    "            type=int,\n",
    "            default=-1,\n",
    "            help=\"number of classes or regression targets\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--init-token\",\n",
    "            type=int,\n",
    "            default=None,\n",
    "            help=\"add token at the beginning of each batch item\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--separator-token\",\n",
    "            type=int,\n",
    "            default=None,\n",
    "            help=\"add separator token between inputs\",\n",
    "        )\n",
    "        parser.add_argument(\"--regression-target\", action=\"store_true\", default=False)\n",
    "        parser.add_argument(\"--no-shuffle\", action=\"store_true\", default=False)\n",
    "        parser.add_argument(\n",
    "            \"--shorten-method\",\n",
    "            default=\"none\",\n",
    "            choices=[\"none\", \"truncate\", \"random_crop\"],\n",
    "            help=\"if not none, shorten sequences that exceed --tokens-per-sample\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--shorten-data-split-list\",\n",
    "            default=\"\",\n",
    "            help=\"comma-separated list of dataset splits to apply shortening to, \"\n",
    "                 'e.g., \"train,valid\" (default: all dataset splits)',\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--add-prev-output-tokens\",\n",
    "            action=\"store_true\",\n",
    "            default=False,\n",
    "            help=\"add prev_output_tokens to sample, used for encoder-decoder arch\",\n",
    "        )\n",
    "\n",
    "        #####\n",
    "        parser.add_argument(\n",
    "            \"--max-positions\", type=int, help=\"number of positional embeddings to learn\"\n",
    "        )\n",
    "        parser.add_argument('--langs', required=True, metavar='LANG',\n",
    "                            help='comma-separated list of monolingual language, '\n",
    "                                 'for example, \"en,de,fr\". These should match the '\n",
    "                                 'langs from pretraining (and be in the same order). '\n",
    "                                 'You should always add all pretraining language idx '\n",
    "                                 'during finetuning.')\n",
    "\n",
    "    def __init__(self, args, data_dictionary, label_dictionary):\n",
    "        super().__init__(args)\n",
    "        self.dictionary = data_dictionary\n",
    "        self._label_dictionary = label_dictionary\n",
    "        if not hasattr(args, \"max_positions\"):\n",
    "            self._max_positions = (\n",
    "                args.max_source_positions,\n",
    "                args.max_target_positions,\n",
    "            )\n",
    "        else:\n",
    "            self._max_positions = args.max_positions\n",
    "        args.tokens_per_sample = self._max_positions\n",
    "\n",
    "    @classmethod\n",
    "    def load_dictionary(cls, args, filename, source=True):\n",
    "        \"\"\"Load the dictionary from the filename\n",
    "        Args:\n",
    "            filename (str): the filename\n",
    "        \"\"\"\n",
    "        dictionary = Dictionary.load(filename)\n",
    "        ##\n",
    "        langs = args.langs.split(\",\")\n",
    "        for l in langs:\n",
    "            dictionary.add_symbol(\"[{}]\".format(l))\n",
    "        dictionary.add_symbol(\"<mask>\")\n",
    "        return dictionary\n",
    "\n",
    "    @classmethod\n",
    "    def setup_task(cls, args, **kwargs):\n",
    "        assert args.num_classes > 0, \"Must set --num-classes\"\n",
    "\n",
    "        # load data dictionary\n",
    "        data_dict = cls.load_dictionary(\n",
    "            args,\n",
    "            os.path.join(args.data, \"input0\", \"dict.txt\"),\n",
    "            source=True,\n",
    "        )\n",
    "        logger.info(\"[input] dictionary: {} types\".format(len(data_dict)))\n",
    "\n",
    "        label_dict = None\n",
    "        if not args.regression_target:\n",
    "            # load label dictionary\n",
    "            label_dict = cls.load_dictionary(\n",
    "                args,\n",
    "                os.path.join(args.data, \"label\", \"dict.txt\"),\n",
    "                source=False,\n",
    "            )\n",
    "            logger.info(\"[label] dictionary: {} types\".format(len(label_dict)))\n",
    "        else:\n",
    "            label_dict = data_dict\n",
    "        return cls(args, data_dict, label_dict)\n",
    "\n",
    "    def load_dataset(self, split, combine=False, **kwargs):\n",
    "        \"\"\"Load a given dataset split (e.g., train, valid, test).\"\"\"\n",
    "\n",
    "        def get_path(key, split):\n",
    "            return os.path.join(self.args.data, key, split)\n",
    "\n",
    "        def make_dataset(key, dictionary):\n",
    "            split_path = get_path(key, split)\n",
    "\n",
    "            dataset = data_utils.load_indexed_dataset(\n",
    "                split_path,\n",
    "                dictionary,\n",
    "                self.args.dataset_impl,\n",
    "                combine=combine,\n",
    "            )\n",
    "            return dataset\n",
    "\n",
    "        input0 = make_dataset(\"input0\", self.source_dictionary)\n",
    "        assert input0 is not None, \"could not find dataset: {}\".format(\n",
    "            get_path(\"input0\", split)\n",
    "        )\n",
    "        input1 = make_dataset(\"input1\", self.source_dictionary)\n",
    "\n",
    "        if self.args.init_token is not None:\n",
    "            input0 = PrependTokenDataset(input0, self.args.init_token)\n",
    "\n",
    "        if input1 is None:\n",
    "            src_tokens = input0\n",
    "        else:\n",
    "            if self.args.separator_token is not None:\n",
    "                input1 = PrependTokenDataset(input1, self.args.separator_token)\n",
    "\n",
    "            src_tokens = ConcatSentencesDataset(input0, input1)\n",
    "\n",
    "        with data_utils.numpy_seed(self.args.seed):\n",
    "            shuffle = np.random.permutation(len(src_tokens))\n",
    "\n",
    "        src_tokens = maybe_shorten_dataset(\n",
    "            src_tokens,\n",
    "            split,\n",
    "            self.args.shorten_data_split_list,\n",
    "            self.args.shorten_method,\n",
    "            self.max_positions(),\n",
    "            self.args.seed,\n",
    "        )\n",
    "\n",
    "        dataset = {\n",
    "            \"id\": IdDataset(),\n",
    "            \"net_input\": {\n",
    "                \"src_tokens\": RightPadDataset(\n",
    "                    src_tokens,\n",
    "                    pad_idx=self.source_dictionary.pad(),\n",
    "                ),\n",
    "                \"src_lengths\": NumelDataset(src_tokens, reduce=False),\n",
    "            },\n",
    "            \"nsentences\": NumSamplesDataset(),\n",
    "            \"ntokens\": NumelDataset(src_tokens, reduce=True),\n",
    "        }\n",
    "\n",
    "        if self.args.add_prev_output_tokens:\n",
    "            prev_tokens_dataset = RightPadDataset(\n",
    "                RollDataset(src_tokens, 1),\n",
    "                pad_idx=self.dictionary.pad(),\n",
    "            )\n",
    "            dataset[\"net_input\"].update(\n",
    "                prev_output_tokens=prev_tokens_dataset,\n",
    "            )\n",
    "\n",
    "        if not self.args.regression_target:\n",
    "            label_dataset = make_dataset(\"label\", self.label_dictionary)\n",
    "            if label_dataset is not None:\n",
    "                dataset.update(\n",
    "                    target=OffsetTokensDataset(\n",
    "                        StripTokenDataset(\n",
    "                            label_dataset,\n",
    "                            id_to_strip=self.label_dictionary.eos(),\n",
    "                        ),\n",
    "                        offset=-self.label_dictionary.nspecial,\n",
    "                    )\n",
    "                )\n",
    "        else:\n",
    "            label_path = \"{0}.label\".format(get_path(\"label\", split))\n",
    "            if os.path.exists(label_path):\n",
    "                def parse_regression_target(i, line):\n",
    "                    values = line.split()\n",
    "                    assert (\n",
    "                            len(values) == self.args.num_classes\n",
    "                    ), f'expected num_classes={self.args.num_classes} regression target values on line {i}, found: \"{line}\"'\n",
    "                    return [float(x) for x in values]\n",
    "\n",
    "                with open(label_path) as h:\n",
    "                    dataset.update(\n",
    "                        target=RawLabelDataset(\n",
    "                            [\n",
    "                                parse_regression_target(i, line.strip())\n",
    "                                for i, line in enumerate(h.readlines())\n",
    "                            ]\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "        nested_dataset = NestedDictionaryDataset(\n",
    "            dataset,\n",
    "            sizes=[src_tokens.sizes],\n",
    "        )\n",
    "\n",
    "        if self.args.no_shuffle:\n",
    "            dataset = nested_dataset\n",
    "        else:\n",
    "            dataset = SortDataset(\n",
    "                nested_dataset,\n",
    "                # shuffle\n",
    "                sort_order=[shuffle],\n",
    "            )\n",
    "\n",
    "        logger.info(\"Loaded {0} with #samples: {1}\".format(split, len(dataset)))\n",
    "\n",
    "        self.datasets[split] = dataset\n",
    "        return self.datasets[split]\n",
    "\n",
    "    def build_model(self, args):\n",
    "        from fairseq import models\n",
    "\n",
    "        model = models.build_model(args, self)\n",
    "\n",
    "        model.register_classification_head(\n",
    "            getattr(args, \"classification_head_name\", \"sentence_classification_head\"),\n",
    "            num_classes=self.args.num_classes,\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "    def max_positions(self):\n",
    "        return self._max_positions\n",
    "\n",
    "    @property\n",
    "    def source_dictionary(self):\n",
    "        return self.dictionary\n",
    "\n",
    "    @property\n",
    "    def target_dictionary(self):\n",
    "        return self.dictionary\n",
    "\n",
    "    @property\n",
    "    def label_dictionary(self):\n",
    "        return self._label_dictionary\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "plbart = BARTModel.from_pretrained('PLBART/plbart-c-cpp-defect-detection', checkpoint_file='model.pt')\n",
    "plbart.eval()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "BARTHubInterface(\n",
       "  (model): BARTModel(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (dropout_module): FairseqDropout()\n",
       "      (embed_tokens): Embedding(50005, 768, padding_idx=1)\n",
       "      (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (dropout_module): FairseqDropout()\n",
       "      (embed_tokens): Embedding(50005, 768, padding_idx=1)\n",
       "      (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerDecoderLayer(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (output_projection): Linear(in_features=768, out_features=50005, bias=False)\n",
       "    )\n",
       "    (classification_heads): ModuleDict(\n",
       "      (sentence_classification_head): BARTClassificationHead(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "len(plbart.task.source_dictionary)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "50005"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "for i in [0,1,2,3,50001,50002, 50003, 50004]:\n",
    "    print(i, plbart.task.source_dictionary[i])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 <s>\n",
      "1 <pad>\n",
      "2 </s>\n",
      "3 <unk>\n",
      "50001 [java]\n",
      "50002 [python]\n",
      "50003 [en_XX]\n",
      "50004 <mask>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "fs_model = plbart.model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "plbart.args"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Namespace(activation_dropout=0.0, activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_prev_output_tokens=True, all_gather_list_size=16384, arch='mbart_base', attention_dropout=0.1, best_checkpoint_metric='accuracy', bf16=False, bpe='gpt2', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', classification_head_name='sentence_classification_head', clip_norm=1.0, cpu=False, criterion='sentence_prediction', cross_self_attention=False, curriculum=0, data='/home/crocoder/Desktop/transformers/PLBART/plbart-c-cpp-defect-detection', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', decoder_attention_heads=12, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=768, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, fast_stat_sync=False, find_unused_parameters=True, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gpt2_encoder_json='https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json', gpt2_vocab_bpe='https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe', init_token=0, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, langs='java,python,en_XX', layernorm_embedding=True, load_checkpoint_heads=True, localsgd_frequency=3, log_format='json', log_interval=10, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=5, max_positions=512, max_sentences=4, max_sentences_valid=4, max_source_positions=1024, max_target_positions=1024, max_tokens=2048, max_tokens_valid=2048, max_update=15000, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_shuffle=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_classes=2, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, regression_target=False, relu_dropout=0.0, required_batch_size_multiple=1, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, restore_file='/local/wasiahmad/workspace/projects/PLBART/pretrain/checkpoint_11_100000.pt', save_dir='/local/wasiahmad/workspace/projects/PLBART/scripts/code_to_code/defect_prediction/devign', save_interval=1, save_interval_updates=0, seed=1234, sentence_avg=False, separator_token=None, share_all_embeddings=True, share_decoder_input_output_embed=True, shorten_data_split_list='', shorten_method='truncate', skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_time_hours=0, task='plbart_sentence_prediction', tensorboard_logdir='', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tokens_per_sample=512, total_num_update=1000000, tpu=False, train_subset='train', update_freq=[4], use_bmuf=False, use_old_adam=False, user_dir='/local/wasiahmad/workspace/projects/PLBART/source', valid_subset='valid', validate_interval=1, warmup_updates=500, weight_decay=0.0)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "from transformers import PLBartConfig, PLBartForSequenceClassification"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "hf_model = PLBartForSequenceClassification.from_pretrained('plbart-c-cpp-defect-detection')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inputs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "import sentencepiece as spm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "vocab_filepath = \"./PLBART/plbart_orig_pretrained_ckpt/sentencepiece.bpe.model\"\n",
    "tokenizer = spm.SentencePieceProcessor()\n",
    "tokenizer.Load(vocab_filepath)\n",
    "tokenizer.SetEncodeExtraOptions(\"\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "texts = [\"This is a sample text\", \"Another example here\"]\n",
    "text_0_tokens = tokenizer.EncodeAsIds(texts[0].strip())\n",
    "text_1_tokens = tokenizer.EncodeAsIds(texts[1].strip())\n",
    "# Need to check how is the original tokenizer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "text_1_tokens += [1]*2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "input_ids = torch.from_numpy(np.array([text_0_tokens, text_1_tokens]))\n",
    "attention_mask = torch.ones_like(input_ids)\n",
    "attention_mask[1, -2:] = 0\n",
    "token_type_ids = torch.zeros_like(input_ids)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Encoder Embeddings"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fairseq"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "fs_embeds, embed = fs_model.encoder.forward_embedding(input_ids)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# fs_model.encoder.quant_noise # None\n",
    "# fs_model.encoder.layernorm_embedding #  not None\n",
    "# fs_model.encoder.embed_positions # not None"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## HuggingFace"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "hf_model.model.encoder.training"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "input_shape = input_ids.size()\n",
    "inputs_embeds = hf_model.model.encoder.embed_tokens(input_ids) * hf_model.model.encoder.embed_scale\n",
    "embed_pos = hf_model.model.encoder.embed_positions(input_shape)\n",
    "hf_embeds = inputs_embeds + embed_pos\n",
    "hf_embeds = hf_model.model.encoder.layernorm_embedding(hf_embeds)\n",
    "hf_embeds = nn.functional.dropout(hf_embeds, p= hf_model.model.encoder.dropout, training= hf_model.model.encoder.training)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "torch.allclose(hf_embeds[0], fs_embeds[0], atol=1e-5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "torch.allclose(hf_embeds[1,:-2], fs_embeds[1, :-2], atol=1e-5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "hf_embeds[1, -2:]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-0.8661,  0.3116,  1.0590,  ..., -0.0160,  0.5052, -1.4175],\n",
       "        [-0.9077,  0.2613,  0.8916,  ...,  0.3555,  0.6211, -1.4142]],\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "fs_embeds[1, -2:] # Fairseq handles padding tokens differently, so they won't match."
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-0.6240,  0.1483,  1.1706,  ...,  0.1917,  0.5521, -1.0526],\n",
       "        [-0.6240,  0.1483,  1.1706,  ...,  0.1917,  0.5521, -1.0526]],\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "torch.allclose(hf_embeds[1,-2:], fs_embeds[1, -2:], atol=1e-5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Encoder Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fairseq"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "fs_encoder = fs_model.encoder"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "src_lengths = torch.tensor([len(text_0_tokens), len(text_1_tokens)])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "fs_encoder_out = fs_encoder(input_ids, src_lengths)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "fs_encoder_out_encoder_out = fs_encoder_out.encoder_out.permute(1, 0, 2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "fs_encoder_out_encoder_out.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 768])"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## HuggingFace"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "hf_encoder = hf_model.model.encoder"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "hf_encoder_out = hf_encoder(input_ids, attention_mask)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "hf_encoder_out = hf_encoder_out.last_hidden_state"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "torch.allclose(hf_encoder_out[0], fs_encoder_out_encoder_out[0], atol=1e-5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "torch.allclose(hf_encoder_out[1,:-2], fs_encoder_out_encoder_out[1,:-2], atol=1e-5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Decoder"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fairseq"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "fs_decoder = fs_model.decoder"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "fs_decoder_out = fs_decoder(input_ids, fs_encoder_out)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "len(fs_decoder_out)\n",
    "fs_decoder_out_decoder_out = fs_decoder_out[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "fs_decoder_out[1].keys()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['attn', 'inner_states'])"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "#  fs_decoder_out[1]['inner_states'][-1].permute(1, 0, 2).shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "fs_decoder_out_inner_state = fs_decoder_out[1]['inner_states'][-1].permute(1, 0, 2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "fs_decoder_out_decoder_out.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 50005])"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## HuggingFace"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "hf_decoder = hf_model.model.decoder"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "hf_decoder_out = hf_decoder(input_ids, attention_mask=attention_mask, encoder_hidden_states=hf_encoder_out, encoder_attention_mask=attention_mask)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "hf_decoder_out.last_hidden_state.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 768])"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "torch.allclose(hf_decoder_out.last_hidden_state[0], fs_decoder_out_inner_state[0], atol=1e-5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "torch.allclose(hf_decoder_out.last_hidden_state[1, :-2], fs_decoder_out_inner_state[1, :-2], atol=1e-5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "torch.allclose(hf_decoder_out.last_hidden_state[1], fs_decoder_out_inner_state[1], atol=1e-5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classification Head"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fairseq"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "logits = fs_model.classification_heads.sentence_classification_head(fs_decoder_out_inner_state[:, -1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "logits.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## HuggingFace"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "clf_logits = hf_model.classification_head(hf_decoder_out.last_hidden_state[:, -1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "hf_decoder_out.last_hidden_state[:, -1]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-0.0129, -0.0352, -0.0964,  ..., -0.0557,  0.2081,  0.3102],\n",
       "        [ 0.1923,  0.1242, -0.0606,  ...,  0.0995,  0.1881,  0.5741]],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "fs_decoder_out_inner_state[:, -1]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-0.0129, -0.0352, -0.0964,  ..., -0.0557,  0.2081,  0.3102],\n",
       "        [ 0.1923,  0.1242, -0.0606,  ...,  0.0995,  0.1881,  0.5741]],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "clf_logits.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "torch.allclose(clf_logits[0], logits[0], atol=1e-5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "torch.allclose(clf_logits[1], logits[1], atol=1e-5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('plbart_env': conda)"
  },
  "interpreter": {
   "hash": "ba545678379e249e1c100136bf9142ff6c40d0b4d28bda3546d9eb7ad34a3e82"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}